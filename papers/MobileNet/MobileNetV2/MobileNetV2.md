# MobileNetV2: Inverted Residuals and Linear Bottlenecks

基于一种逆残差结构，其shortcut连接位于窄bottleneck层间。中间的扩张层使用轻量的深度卷积过滤特征，其作为非线性的源。额外地，我们发现为了保持表达能力，在窄层移除非线性部分很重要。



## 1. Introduction

本文介绍一个专门为移动设备和资源受限环境剪裁的新的神经网络架构。

主要贡献是一个新的层的模块：带线性bottleneck的逆残差结构。这一模块接收一个压缩后的低维输入，首先扩张成高维然后经过一个轻量的深度卷积进行滤波。随后使用一个线性卷积将特征映射回地位表示。



## 2. Related Work

blahblahblah



## 3. Preliminaries, discussion and intuition

### 3.1. Depthwise Separable Convolutions

深度可分离卷积是很多高效神经网络架构的关键组成块。基本的思路是将全卷积操作换成一个分解后的卷积，它将卷积分成两个分离的层。第一层称为深度卷积，通过在每个通道使用一个滤波器实现轻量级的滤波操作。第二个层是一个$1\times1$的卷积层，称为逐点卷积，它通过计算输入通道的线性组合来生成新特征。

标准的卷积操作接收$h_i\times w_i\times d_i$的输入张量$L_i$，使用$K\in \mathcal{R}^{k\times k\times d_i \times d_j}$的卷积核生成一个$h_i\times w_i\times d_j$的输出张量$L_j$.标准卷积层的计算量为$h_i\cdot w_i\cdot d_i\cdot d_j\cdot k\cdot k$.

深度可分离卷积是标准卷积可随时使用的替换方案。经验上来看，二者表现相当，但是深度可分离卷积的计算量仅为:
$$
h_i\cdot w_i\cdot d_i(k^2+d_j) \quad\quad\quad(1)
$$
这是深度卷积和$1\times1$逐点卷积的和。实际上和传统卷积相比，深度可分离卷积减少计算量的因子和$k^2$差不多（准确的说是$k^2d_j/(k^2+d_j)$）。MobileNetV2使用$k=3$（$3\times3$深度可分离卷积），所以计算量要比标准卷积少8到9倍，而精度的下降却很少。

### 3.2. Linear Bottlenecks

考虑一个由$n$层$L_i$构成的深度神经网络，其中每一层都有一个维度为$h_i\times w_i\times d_i$的激活张量。本节会讨论这些激活张量的基本性质，将其看成有$h_i\times w_i$个像素，$d_i$个维度的容器。非正式地，对于一个输入图片的集合，称激活层的集合（对任意层$L_i$）组成一个"manifolds of interest".长期以来人们一直认为神经网络中的manifolds of interest可以被嵌入到低维子空间。换句话说，当我们看一个深度卷积层的所有独立d通道像素的时候，这些值所编码的信息实际上位于某些流形上，也就是可嵌入到一个低维的子空间中（注意到流形的维度与子空间的维度是不同的，可以通过一个线性变换进行转换）。

打眼一看，这样的一个事实可以通过对层操作空间进行简单地降维实现。MobileNetV1中成功利用了这一点，使用一个宽度乘数参数在计算量和精准度间进行有效的trade off，也被其他网络用于高效的模块设计当中。跟随这一直观理解，宽度乘数允许我们降低激活空间的维度直到manifold of interest张成整个空间。但是，当我们想起深度卷积神经网络实际上每个坐标变换都有非线性模块（如ReLU）时，这个直观理解就崩溃了。例如，在1维空间中ReLU作用于一条直线上生成一个射线，对于$\mathcal{R}^n$空间来说，基本上会得到一个有n个节点的分段曲线。

![Figure 1](1.png"Figure 1")

*低维流形嵌入到高维空间的ReLU变换的例子。这些样例的初始螺旋线是用随机矩阵$T$嵌入在n维空间中的，后接ReLU，然后使用$T^{-1}$映射回2D空间。上面的例子中$n=2,3$的结果是流形的某些点坍塌到一起去了，对于$n=15to30$的变换则是高度非凸的。*



一般来说如果某一层的ReLU变换的结果（$Bx$）有一个非零值 $S$，映射到interior $S$的点就是输入经过一个线性变换$B$得到的，也就代表着输入空间与完整维度的输出相关的那一部分，受一个线性变换限制。换句话说，深度网络只有在输出域的非零值部分上进行线性分类的能力。*见补充材料*

另一方面，当ReLU使通道坍塌时，必然会丢失这个通道的信息。但是如果我们很多通道的话，激活流形中有一种结构也许可以让信息仍然被保留在其他通道中。补充材料中展示了如果输入流形能被嵌入到激活空间的一个低很多的低维子空间中，那么ReLU变换就能在向可表示函数集合中引入所需复杂度的同时保留信息。

总的来说，我们强调了两个性质，这两个性质是由需求所指出的，需求是manifold of interest应该位于更高维的激活空间的一个低维子空间中。两个性质为：

1. 如果manifold of interest在ReLU变换后是非零值（volume），它就相当于一个线性变换。
2. 当且仅当输入流形在输入空间的一个低维子空间中时，ReLU有能力保留关于输入流形的全部信息。

这两点理解给我们提供了一个优化现有神经网络架构的经验上的提示：假设manifold of interest是低维的，我们可以通过在卷积块中插入linear bottleneck层来获取它。试验结果表明，使用线性层是很有必要的，因为它阻止了非线性模块毁掉太多信息。

本文会使用bottleneck卷积。将输入bottleneck的尺寸和内部尺寸之比称为膨胀比（expansion ratio）

![Figure 2](2.png"Figure 2")



### 3.3. Inverted residuals

bottleneck块和残差块类似，每个块都包含一个输入，后接几个bottlenecks然后是扩张。但是受bottleneck实际包含所有必要信息这一直觉的启发，而一个扩张层仅相当于伴随张量的非线性变换的一个实现细节，直接在bottlenecks间使用shortcuts.

![Figure 3](3.png"Figure 3")

Figure 3简略地展示了设计上的差异。加入shortcuts的动机与经典的残差连接类似：想要提升一个梯度跨越多层传播的能力。但是逆向结构的设计在内存占用上有大幅度降低，同样在实验中的效果也有一点提升。

**Running time and parameter count for bottleneck convolution**

基础实现结构如Table 1中所示。

![Table 1](t1.png"Table 1")

一个尺寸为$h\times w$的块，扩张因子$t$和kernel size $k$，输入通道数为$d'$，输出通道数为$d''$，其乘加计算量为$h\cdot w\cdot d'\cdot t(d'+k^2+d'')$.与公式(1)相比，这个表达式多了一项，因为实际上我们用了额外的$1\times1$卷积，但是我们的网络的特性允许我们利用更小的输入和输出维度。Table 3中比较了MobileNetV1，MobileNetV2和ShuffleNet对不同分辨率所需的尺寸。

![Table 2](t2.png"Table 2")

![Table 3](t3.png"Table 3")



### 3.4. Information flow interpretation

这种架构的一个有趣的特性是它在building blocks和layer transformation（是一个非线性函数，将输入转换成输出）的输入域和输出域之间提供了一个自然的隔离。前者可视为网络每一层的容量，后者可视为表达能力。与传统卷积块相比，无论一般的还是可分离的，它们的表达能力和容量纠缠到一起且是输出层深度的函数。

实际上，我们的方案中，当内部层深度为0则代表其基础卷积是identity函数。当扩张率小于1，就是一个经典的残差卷积块。

这一解释使我们能独立于网络容量去研究网络的表达能力，我们相信有关这种分离的进一步的探索可以保证对网络性质的更好理解。



## 4. Model Architecture

前面说到基础的building block是一个带残差的bottleneck深度可分离卷积，其结构细节可见Table 1.MobileNetV2的架构包括初始含32个滤波核的全卷积层，接着是19个残差bottleneck层，见Table 2.使用ReLU6作为非线性函数，因为其在低精度计算中的鲁棒性。总是使用$3\times3$的卷积核，因为是现代网络的标配，还在训练过程中使用dropout和batchnorm.

除了第一层之外，在整个网络中使用常数扩张率。实验中发现扩张率在5到10之间时的性能曲线结果差不多，小网络最好使用稍微小一点的扩张率，大网络使用稍大的扩张率效果会更好。

我们的所有主要实验中使用的扩张因子都为6.例如对于一个接收64通道张量作为输入，且输出128个通道的bottleneck layer，其内部扩张层就是$64\cdot6=384$个通道。



**Trade-off hyper parameters**

