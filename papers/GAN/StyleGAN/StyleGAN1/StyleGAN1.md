# A Style-Based Generator Architecture for Generative Adversarial Networks

**Abstract** 提出一个用于生成对抗网络的可替代的生成器架构，借鉴于风格迁移作品。新架构能够得到自动学习，无监督的分离的高级特征（例如用于人脸生成中的姿态、id等）和生成的图片中的随机变量（如雀斑、头发），并对合成具有直观且尺度特定的控制能力。新的生成器在传统分布质量度量上达到sota，明确地更好的内部属性，同时更好地对变量潜在的影响因素进行解耦。



# 1. Introduction

GAN生成的图像的分辨率和质量持续提升。但是生成器仍然如黑盒一样，仍然缺少对图片合成过程中的不同方面的理解，如初始随机特征等。人们对隐空间（latent space）的理解同样很少，常见的展示隐空间的方法（? interpolations）也没有提供比较不同生成器的量化方法。

受风格迁移启发，重新设计了生成器的架构，暴露新方式用于控制图像合成过程。我们的生成器从一个学习后的常量输入开始，没一个卷积层基于隐编码调整图像的“风格”，从而直接控制不同尺度上图像特征的强度。与直接注入网络的噪音结合，这个结构上的改变获得了随机变量的高级属性的自动的、无监督的分离。并没有改变判别器和loss函数。

将输入的隐编码嵌入到一个中间隐空间中，这样做对网络如何表达变量有很大效果。输入隐空间必须遵循训练数据的概率密度，但是我们主张这一结论在某种程度上会导致无法避免的耦合。我们的中间隐空间就不受这个限制，因此可以解耦。由于以前估计隐空间解耦程度的方法不能直接应用于我们的案例，所以提出两个新的自动度量：**感知路径长度**和**线性分离度**，用于量化生成器的这些方面。



# 2. Style-based generator

一般来讲都是通过一个输入层将隐编码传入生成器的，例如前向网络的第一层，如Fig 1a所示。我们不采用这种方法，而是将输入层全部省略，从一个可学习的常量开始，如Fig 1b所示。给定一个属于输入隐空间$\mathcal{Z}$的隐编码$\textbf{z}$,首先经过一个非线性映射网络$f:\mathcal{Z} \to \mathcal{W}$生成$\textbf{w}\in \mathcal{W}$. 简单起见，将两个空间的维度都设定为512，映射$f$使用的是一个8层的MLP. 