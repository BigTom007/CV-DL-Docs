# Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition

## 1 Introduction

问题：人脸数据集人工标注时间成本太高，几乎所有数据集都有噪音问题

为了解决上述问题，我们将视角从获取更多人工标注的标签转向利用更多无标签的数据上。和大规模id标注不同，无标签的图片很容易获取。比如，用一个人脸检测器加爬虫软件就可以轻易获取大量的非限制场景人脸图片或视频。现在的主要问题变成了如何利用已有的大量无标签数据来提高大规模人脸识别效果。

我们的不表示最大化利用无标签数据，使最终的性能表现与使用标注数据的性能表小相近。这里一个关键结论是尽管无标签数据无法提供直接的语义类别，但是其可用图来表示的内部结构，实际上反映了人脸表示的高维分布。

众所周知，由单独一个模型提出的特征通常容易受到偏置的影响，且对噪声敏感。为解决这个问题，使用一个bottom-up方法，通过首先可靠地识别正样本对的方式来构建图。称为CDP（共识驱动传播，Consensus-Driven Propagation），由两个模块组成：一个“**委员会**(committee)”，用于提供样本对的多角度信息；和一个“**中介**(mediator)”，将全部信息聚合成最终决策结果。

“**委员会**”模块受QBC(query-by-committee)启发，QBC原本用于激活学习。和QBC用于衡量差异不同，我们从由一个基础模型和多个辅助模型的委员会收集consents. committee的异构特性在无标签数据的结构上表现出多个不同视角。将委员会中多数成员赞同的样本对选作正样本对，而不是根据基础模型的置信度选择。因此委员会模块能够从无标签图像中选择有意义的和困难的正样本对，而不是只选出简单样本。除了投票机制之外，我们使用一种新的更有效的“**中介**”来聚合委员会的意见。中介是一个二分类器，其生成是否选择一个样本对的最终决策。我们仔细设计了中介的输入，使之能够覆盖内部结构的分布信息。其输入包括：1）委员会的投票结果，2）样本对之间的相似度，3）样本对之间的局部密度。最后两个输入是通过委员会的全部成员和基础模型衡量的。使用“委员会”和“中介”模块，我们对无标签数据构建了一个鲁棒的共识驱动的图。最终在图上传播伪标签，形成一个辅助任务用来使用无标签数据训练基础模型。



## 2 Related Work

**Semi-supervised Face Recognition.** 在已有少量标注数据的前提下，为了利用大量无标签数据而提出了半监督学习。一般来说，其目的是通过多种方式根据受限的标注数据将标签传播到整个数据集，采用的方法有自训练、协同训练、多视角学习、期望最大化和基于图的方法。

**Query-by-Committee.** QBC是一个依赖于多判别模型来获取不同意见的策略，也因此挖掘机器学习任务的有意义的样本。



## 3 Methodology

首先提出一个总览，本方法主要有三部分组成：

1）**Supervised initialization** - 给定一小部分标注数据，使用全监督的方式独立地训练基础模型和委员会成员。具体来说，基础模型$B$和全部$N$个委员会成员$\{C_i|i=1,2,...,N\}$使用标注好的数据$D_l$学习一个从图像空间到特征空间的映射$\mathcal{Z}$. 对于基础模型，这一过程可表示为：$\mathcal{F}:D_l \mapsto \mathcal{Z}$, 对委员会成员为：$\mathcal{F}_{C_i}:D_l \mapsto \mathcal{Z}, i=1,2,...,N$.

2）**Consensus-driven propagation** - 在无标签数据上使用CDP来选择有价值的样本，并在此基础上推测其标签。其架构见Fig 1. 我们使用第一阶段训练好的模型来提取无标签数据的深度特征，并创建k-NN图来选择有意义的样本对。使用选中的无标签样本对来构建一个共识驱动图，并使用我们的标签传播算法将伪标签分配个各结点。

3）**Joint training using labeled and unlabeled data** - 最终，在多任务学习框架下，使用有标签数据和伪标签数据重新训练基础模型。

![Figure 1](1.png"Figure 1")

### 3.1 Consensus-Driven Propagation

本节介绍CDP的细节步骤。

**i. Building k-NN Graphs.** 

将无标签数据$D_u$作为输入传入基础模型和委员会成员，提取到深度特征$\mathcal{F}_B(D_u)$和$\mathcal{F}_{C_i}(D_u)$. 使用特征找到$D_u$中每个样本余弦相似度的k最近邻。这样得到了不同版本的k-NN图，$\mathcal{G}_B$对应基础模型，$\mathcal{G}_{C_i}$对应每个委员会成员，一共$N+1$个图。图中的节点代表无标签数据的样本。每一条边定义了一对，基础模型对应图$\mathcal{G}_B$的所有的对构成了后续选择操作的候选者，如Fig 1所示。

**ii. Collecting Opinions from Committee.** 

委员会成员通过使用映射函数$\{\mathcal{F}_{C_i}|i=1,2,...,N\}$将无标签数据映射到特征空间。假设由基础模型创建的图中有任意两个连接的节点$n_0$和$n_1$，它们由不同版本的深度特征$\{\mathcal{F}_{C_i}(n_0)|i=1,2,...,N\}$和$\{\mathcal{F}_{C_i}(n_1)|i=1,2,...,N\}$表示。委员会提供了下述因素：

1）*关系*，两个节点之间的关系$R$. 直观上可以理解为两个节点再每一个委员会成员的视角下是否相邻。
$$
R_{C_i}^{(n_0,n_1)}=
\begin{cases}
1 \quad \mathrm{if} (n_0, n_1)\in \varepsilon(\mathcal{G}_{C_i}) \\
0 \quad \mathrm{otherwise.}
\end{cases}, \quad i=1,2,...,N, \qquad (1)
$$
其中$\mathcal{G}_{C_i}$是第$i$个委员会模型的k-NN图，$\varepsilon$代表图的全部边。

2）*亲和度*，两个节点间的亲和度$A$. 可以通过计算特征空间的相似度得到，映射函数由委员会成员定义。假设我们使用余弦相似度，
$$
A_{C_i}^{(n_0,n_1)}=\cos(\langle \mathcal{F}_{C_i}(n_0), \mathcal{F}_{C_i}(n_1) \rangle), \quad i=1,2,...,N. \qquad (2)
$$
3）每个节点的*局部结构*。这个概念可以参考一个节点的第一级、第二级甚至更高级邻居的分布。其中第一级邻居扮演了表示一个节点“局部结构”中最重要的角色。这样的分布可以近似为节点$x$和其所有相邻节点$x_k$的相似度分布，其中$k=1,2,...,K$.
$$
D_{C_i}^x=\{\cos(\langle \mathcal{F}_{C_i},\mathcal{F}_{C_i} \rangle), k=1,2,...,K\}, \quad i=1,2,...,N. \qquad (3)
$$
![Figure 2](2.png"Figure 2")





























