# Online Deep Clustering for Unsupervised Representation Learning

## Abstract

联合聚类和特征学习方法在无监督表示学习中展现出了强大的能力。但是，其训练计划在特征聚类和更新网络参数之间的切换会导致视觉表达的不稳定的学习。为了解决这一问题，提出在线深度聚类(ODC, Online Deep Clustering)，同时进行聚类和更新网络的操作。关键思想是聚类中心应该在分类器稳定更新的时候保持稳定。特别地，我们设计了两个动态记忆模块，即用于存储样本的标签和特征的采样记忆模块和用于中心演变的中心记忆模块。将粗暴的全局聚类拆分成稳定的记忆更新和按batch的标签重分配。这个流程整合到了网络更新的迭代中。

![Figure 1](1.png"Figure 1")

## 1. Introduction

无监督表示学习的目标是在不许手动标注的条件下学习到图像或者视频的可转移表达。其中基于聚类的表示学习方法成为了这一领域有前景的方向。与基于复原的方法不同，基于聚类的方法只需要少量的域只是就可以达到较好的效果。与对比表达学习只捕获图片内不变性相比，基于聚类的方法能够探索图片间的相似度。与通常作用与固定特征之上的传统聚类相比，这些工作同时进行聚类和特征学习。

尽管早期作品几乎都在小数据集上进行验证，Caron et al.提出的Deep Clustering(DC)第一个尝试提升基于聚类的表达学习的规模。DC在特征聚类和CNN参数更新之间切换。特别地，在每个epoch开始的时候，DC在整个数据集上执行离线聚类算法来获得伪标签作为下一个epoch的监督信号。离线聚类必然会改变不同epochs的标签，也就是说就算有些聚类不发生变化，它们的索引也会在聚类后被随机修改。结果就是当前分类器的参数无法继承到下一个epoch，因此每个epoch开始时分类器的参数就得进行随机初始化。这样的机制引入了训练的不稳定性并将表达暴露在了representation corruption的风险下。如Figure 1(a)所示，每个epoch中，DC的网络更新被特征提取和聚类打断了。与此相对，传统的有监督分类，采用使用固定标签的无中断方式，其每次迭代都由网络的前向和反向传播组成。

本作中想要设计一个高稳定性的联合聚类和特征学习的范式。为了减少训练机制在DC和监督学习间的矛盾，将聚类进程分解为mini-batch级的标签更新，并将这一更新过程融合到网络更新的迭代中。基于这一直觉，提出用于联合聚类和特征学习的在线深度聚类（ODC）。特别地，ODC的一个迭代由前向和反向传播、标签重分配和中心更新组成，因此就避免了额外的特征提取操作。为了帮助在线标签重分配和中心更新，我们设计并维护了两个动态记忆模块，即用于存储样本的标签和特征的采样记忆模块和用于中心演变的中心记忆模块。以这种方式，ODC就可以像有监督分类一样用无中断的方式训练，且不需要手动标注信息。在训练过程中，标签和网络参数同时（而非分别）变化。由于标签是在每次迭代中连续且立刻更新的，CNN中的分类器也同样保持稳定，会得到一个更稳定的loss曲线，如Figure 1(b)所示。

尽管只使用ODC就能达到很好的无监督表示学习效果，同时它也可以用来对使用其他无监督方法训练的模型进行微调。



## 2. Related Work

blahblahblah



## 3. Methodology

### 3.1. Online Deep Clustering

首先探讨一下DC的基本理念，然后展开ODC的细节。为了学习表达，DC需要在离线特征聚类和使用伪标签的网络反向传播之间进行切换。这个离线聚类过程需要在整个训练集上提取深度特征，使用一个全局聚类算法，例如K-Means聚类。这个全局聚类算法在很大程度上替换了伪标签，需要网络在随后的epoch中迅速适应新标签。

**Framework Overview. ** 与DC不同，ODC不需要额外的特征提取过程。另外

