# Online Video Deblurring via Dynamic Temporal Blending Network

SOTA视频去模糊方法可以去除动态场景下由镜头抖动和/或物体移动引起的不均匀的模糊。但是大多数现有方法都基于批处理，因此需要获取全部记录下的视频帧，渲染这些视频帧需要算力很大也很慢，也就限制了其在现实场景下的使用。作为对比，我们提出一个*在线（顺序）*视频去模糊方法，基于时空循环网络(spatio-temporal recurrent network)可以达到实时性能。特别提出一个新的架构，它在扩展感受野的同时保持网络的整体尺寸，以达到快速执行的目的。这样，网络就能去除由相机抖动或者目标快速移动造成的运动模糊。而且还提出一个新的网络层，通过*动态时域混合(dynamic temporal blending)*增强连续帧之间的时间连续性，其方法是比较并适应地共享不同时间步骤的特征。



## 1. Introduction

动态场景下的移动物体和镜头抖动一样，会引起视频的动态模糊，经常会造成视频质量的严重下降。这个问题对于低光照场景尤其严重，因为曝光时间增长了，近些年使用运动的相机（手持）的视频记录变得很受欢迎。因此不仅为了提升视频质量，也为了促进其他视觉任务如追踪，SLAM和密度3D重建等，视频去模糊技术和其应用受到了广泛关注。但是使用盲目的方式（如：每一帧的模糊未知）去除动态模糊和恢复清晰帧是一个高度病态的问题，也是计算摄影学的一个活跃的研究话题。

本文中我们提出一个新的有分辨力的视频去模糊方法。blahblahblah

网络架构使用深度卷积残差网络，其在时间和空间上都是循环的结构。对于时间序列模型，提出一个网络层，实现了一个我们称为动态时域混合的新的机制，它比较连续时间步骤的特征表达，且允许动态（例：输入独立）像素信息传播。空间域的循环是通过一个新的网络层实现的，这个层可以在不增加网络尺寸的前提下增大跨时间感受野的尺寸。这样我们就可以更好的处理更大范围的模糊而无需像其他网络一样有运行时间的限制。

由于缺少可公开获取的训练数据，我们收集了一大批模糊和清洗的视频。我们使用高速摄像机记录下清晰帧，然后通过对几个连续帧进行平均生成真实模糊帧。