# Online Video Deblurring via Dynamic Temporal Blending Network

SOTA视频去模糊方法可以去除动态场景下由镜头抖动和/或物体移动引起的不均匀的模糊。但是大多数现有方法都基于批处理，因此需要获取全部记录下的视频帧，渲染这些视频帧需要算力很大也很慢，也就限制了其在现实场景下的使用。作为对比，我们提出一个*在线（顺序）*视频去模糊方法，基于时空循环网络(spatio-temporal recurrent network)可以达到实时性能。特别提出一个新的架构，它在扩展感受野的同时保持网络的整体尺寸，以达到快速执行的目的。这样，网络就能去除由相机抖动或者目标快速移动造成的运动模糊。而且还提出一个新的网络层，通过*动态时域混合(dynamic temporal blending)*增强连续帧之间的时间连续性，其方法是比较并适应地共享不同时间步骤的特征。



## 1. Introduction

动态场景下的移动物体和镜头抖动一样，会引起视频的动态模糊，经常会造成视频质量的严重下降。这个问题对于低光照场景尤其严重，因为曝光时间增长了，近些年使用运动的相机（手持）的视频记录变得很受欢迎。因此不仅为了提升视频质量，也为了促进其他视觉任务如追踪，SLAM和密度3D重建等，视频去模糊技术和其应用受到了广泛关注。但是使用盲目的方式（如：每一帧的模糊未知）去除动态模糊和恢复清晰帧是一个高度病态的问题，也是计算摄影学的一个活跃的研究话题。

本文中我们提出一个新的有分辨力的视频去模糊方法。blahblahblah

网络架构使用深度卷积残差网络，其在时间和空间上都是循环的结构。对于时间序列模型，提出一个网络层，实现了一个我们称为动态时域混合的新的机制，它比较连续时间步骤的特征表达，且允许动态（例：输入独立）像素信息传播。空间域的循环是通过一个新的网络层实现的，这个层可以在不增加网络尺寸的前提下增大跨时间感受野的尺寸。这样我们就可以更好的处理更大范围的模糊而无需像其他网络一样有运行时间的限制。

由于缺少可公开获取的训练数据，我们收集了一大批模糊和清洗的视频。我们使用高速摄像机记录下清晰帧，然后通过对几个连续帧进行平均生成真实模糊帧。



## 2. Related Work

**Multi-frame Deblurring.** 早期尝试解决由镜头抖动引起的运动模糊考虑使用多张模糊图像和适应方法来去除单一模糊图片中的平均模糊。包括Cai和Zhang在内的其他工作通过利用模糊核的稀疏性和潜在帧的梯度分布来获取清晰帧。最近，Delbracio 和 Sapiro提出傅里叶脉冲累积(FBA, Fourier Burst Accumulation)方法用于脉冲去模糊，这个方法可以在不需要显式指明核的前提下通过将多个傅里叶域的观察的复杂像素系数进行平均完成合并多张模糊图像。Wieschollek等将这一方法使用神经网络进行了扩展，用于单一图像盲去卷积，端到端训练结果很好。

大多数提到的方法都假设了稳定性，例如：变化不变模糊，且无法解决难度更大的空间变化模糊。

**Video Deblurring. **尽管上述的一些方法可以解决由相机抖动引起的不平均模糊，但是它们都不能去除视频中一个动态场景下物体的空间变化模糊主干。更一般地，典型视频中的模糊的可能引起因素有多种，包括运动的物体，相机的抖动，深度的变化，因此需要估计像素级的不同模糊核，这是个非常复杂的问题。

一些早期方法使用清晰的“lucky”帧，在一些情况下存在于长视频中



## 3. Training Datasets

之前的基于学习的单一图像盲反卷积和脉冲去模糊的方法都只考虑了自运动，且假定一个平均模糊模型。但是在空间和时间多样的由自运动和物体运动引起的动态模糊情况下使用这种方法是不直观的。因此我们采用了一个不同的策略，使用了一个最近提出的技术通过一个高速相机生成成对的清晰和模糊的视频。

给定一个高速视频，我们通过平均几个连续短曝光图片来“模拟”长快门时间，这样合成一个视频有更少的长曝光帧。渲染后（平均后）的帧可以代表由相机抖动和/或物体运动引起的动态模糊特征。与此同时，使用中心短曝光图像作为参考的清晰帧，因此有：
$$
\begin{cases}
\textbf{B}_n=\frac{1}{\tau}\sum_{j=1}^\tau \textbf{X}_{nT+j} \\
\textbf{S}_n=\textbf{X}_{nT+\lfloor\frac{\tau}{2}\rfloor}
\end{cases}, \quad\quad\quad (1)
$$
其中$n$代表时序步骤，$\textbf{X}_{nT},\textbf{B}_n,\textbf{S}_n$分别代表短曝光帧，合成的模糊帧和参考清晰帧。参数$\tau$与实际的快门速度相关，决定被平均帧的数量。时间区间$T$，满足$T>\tau$，控制合成视频的帧率。例如，一个帧率为$f$的高速视频的生成视频的帧率为$\frac{f}{T}$.需要注意的是，这个数据集只能用于动态模糊，其他的不行（如：失焦模糊）。可以通过调整$\tau$来控制模糊的强度（大的$\tau$值会生成更模糊的视频），同样可以通过控制时间区间$T$来改变生成视频的占空度(duty cycle)。整个流程见Figure 2

![Figure 2](2.png"Figure 2")



## 4. Method Overview

提出一个视频去模糊的网络，通过模糊帧估计潜在清晰帧。Su等提到，处理视频而非单张图片的直观且朴素的方法是重复地使用一个神经网络，如Figure 3所示。

![Figure 3](3.png"Figure 3")

这里网络的输入是连续的模糊帧$\langle\textbf{B}_n\rangle_m=\{\textbf{B}_{n-m},...,\textbf{B}_{n+m}\}$其中$\textbf{B}_n$是中间帧，$m$是小整数（为简单起见，图中去掉了m）。网络预测step $n$的一个单一清晰帧$\textbf{L}_n$

### 4.1. Spatio-temporal recurrent network

神经网络要想处理大尺度的模糊就必须得有一个大的感受野。例如对尺寸为$101\times101$的模糊核，就需要卷积深度残差网络大概50层$3\times3$小卷积核的卷积层。尽管使用更深的网络或者更大的滤波器尺寸是确保大感受野的直观且简单的办法，但是整体的运行时间会随之增加。因此，我们提出一个高效的网络，既保留了大感受野，又不增加深度和滤波器尺寸。

时间空间网络架构如Figure 3所示，$\textbf{F}_{n-1}$是模糊帧$\langle\textbf{B}_{n-1}\rangle_m$与前一个特征图$\textbf{F}_{n-2}$合并后在第$(n-1)$步计算出的特征图，将$\textbf{F}_{n-1}$作为网络的额外输入与模糊帧$\langle\textbf{B}_n\rangle_m$一同作为第$n$步的输入。这样，在第$n$步模糊帧$\textbf{B}_n$的特征传入同一个网络$(m+1)$次，理想状况下，可以根据同一个因子来增加感受野而不用修改层数和网络参数。注意到实际上感受野的增长是受网络容量限制的。

换句话说，在高维特征空间，每个模糊的输入帧都被我们的网络循环地处理了多遍，从而高效地发现一个使用增大后感受野的更深的空间特征提取方式。而且从前序时间步骤获取的时间信息也经过转移用来增强当前帧，因此我们称这样的网络为*空间时间循环*或者简单的STRCNN.

![Figure 4](4.png"Figure 4")

### 4.2 Dynamic temporal blending network

处理视频而非单一帧的时候，增强时间连续性是很重要的。尽管我们循环地传递前序特征图而且在连续帧间隐式地共享信息，我们开发了一个用于时间信息传播的新机制，可以大幅度提高去模糊的表现。

受最近深度学习方法启发，在测试时网络参数动态地适应输入数据，我们同样为时间特征混合生成权重参数鼓励时域连续性，如Figure 3所示。特别地，基于我们的空间时间循环网络，额外提出一个*动态时间混合*网络，它在时间步骤$n$生成权重参数$\textbf{w}_n$，用于连续时间步骤的特征图的线性混合，
$$
\tilde{\textbf{h}}_n=\textbf{w}_n\otimes\textbf{h}_n+(\textbf{1}-\textbf{w}_n)\otimes\tilde{\textbf{h}}_{n-1}, \quad\quad\quad(2)
$$
其中$\textbf{h}_n$代表当前时间步骤$n$的特征图，$\tilde{\textbf{h}}_n$代表它经过滤波的版本，$\tilde{\textbf{h}}_{n-1}$代表时间步骤$(n-1)$的经过滤波的特征图。权重参数$\textbf{w}_n$的尺寸与$\textbf{h}_n$的尺寸相同，其值在0~1之间。线性操作符$\otimes$代表元素对应相乘，我们的滤波器参数$\textbf{w}_n$可以看作一个局部变化权重图。值得注意的，$\textbf{h}_n$是一个特征，它在整个网络的中部激活，因此与$\textbf{F}_n$不同，后者代表最终的激活。

当$\textbf{h}_n$是削弱版本的$\tilde{\textbf{h}}_{n-1}$，自然更倾向于前序过滤后的特征图$\tilde{\textbf{h}}_{n-1}$.为了实现这个策略，引入一个新单元，它可以通过比较两个特征图的相似度来生成滤波器参数$\textbf{w}_n$，
$$
\textbf{w}_n=\min(\textbf{1},|\tanh(\textbf{A}\tilde{\textbf{h}}_{n-1}+\textbf{B}\tilde{\textbf{h}}_{n})|+\beta) \quad\quad\quad(3)
$$
其中$\tanh(\cdot)$代表双曲正切函数，$\textbf{A}$和$\textbf{B}$对应线性（卷积）滤波器。可训练参数$0\le\beta\le1$代表一个偏置值，它控制混合率，例：当双曲正切函数返回0时，它满足$\textbf{w}_n=\beta$.如Figure 5所示，如果$(\textbf{A}\tilde{\textbf{h}}_{n-1}+\textbf{B}\tilde{\textbf{h}}_{n})$衡量两个特征图的一个恰当的距离时，$\textbf{w}_n$代表三维特征空间里的一个典型惩罚函数。

![Figure 5](5.png"Figure 5")

注意到，为了达到这个目标，只需要一个额外的卷积层和一些非线性激活函数如tanh，因此计算速度很快。尽管提出的动态时间混合网络简单且轻量，但是实验表明它在去模糊质量的提升上帮助很大，我们称这个网络为*STRCNN+DTB*



## 5. Implementation and Training

本节中描述了提出的网络架构的全部细节。如Figure 4所示，其中只展示了一个时间步骤$n$的设置，因为我们的模型共享全部可训练变量。网络由三个模块构成：编码器，动态时间混合网络和解码器。

### 5.1. Network architecture

#### 5.1.1 Encoder

Figure 4(a)描述了网络的编码器。输入为$(2m+1)$连续模糊帧$\langle\textbf{B}_n\rangle_m$其中$\textbf{B}_n$是中间帧，和前一阶段的激活特征$\textbf{F}_{n-1}$.所有输入图像都是彩色且归一化到0~1间。特征图$\textbf{F}_{n-1}$的尺寸是输入图片尺寸的一半，32通道。所有模糊的输入图片先经过卷积层然后和特征图拼接，然后进入一个深度残差网络，使用5个残差块。残差块中的卷积核都是$3\times3\times64$，编码器输出的特征图为$\textbf{h}_n$

#### 5.1.2 Dynamic temporal blending

Figure 4(b)描述了动态时间混合网络。它接收两个拼接后的特征图$\tilde{\textbf{h}}_{n-1}$和$\textbf{h}_n$作为输入，通过一个卷积核为$5\times5$的卷积层和一个后接的压缩函数($\tanh(.)$和$Abs(.)$)。最后生成的权重图$\textbf{w}_n$用于混合$\tilde{\textbf{h}}_{n-1}$和$\textbf{h}_n$

我们通过改变动态时间混合网络的位置测试了不同的布局设置。最好的结果是把动态时间混合网络放在编码器和解码器中间，如Figure 3(c)所示，要比放在编码器或解码器中要好。

#### 5.1.3 Decoder

